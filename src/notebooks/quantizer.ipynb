{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14514042,"sourceType":"datasetVersion","datasetId":9270048},{"sourceId":14514052,"sourceType":"datasetVersion","datasetId":9270054}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U \\\n  transformers \\\n  peft \\\n  accelerate \\\n  bitsandbytes \\\n  datasets\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\nfrom peft import PeftModel\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\nADAPTER_DIR = \"/kaggle/input/adapters2\"\n\nOUTPUT_ROOT = \"./quantized\"\nMERGED_DIR = os.path.join(OUTPUT_ROOT, \"merged-fp16\")\nGGUF_DIR = os.path.join(OUTPUT_ROOT, \"gguf-int4\")\n\nINT8_DIR = os.path.join(OUTPUT_ROOT, \"model-int8\")\nINT4_DIR = os.path.join(OUTPUT_ROOT, \"model-int4\")\n\n\nos.makedirs(MERGED_DIR, exist_ok=True)\nos.makedirs(INT8_DIR, exist_ok=True)\nos.makedirs(INT4_DIR, exist_ok=True)\nos.makedirs(GGUF_DIR, exist_ok=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    BASE_MODEL,\n    trust_remote_code=True\n)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = PeftModel.from_pretrained(model, ADAPTER_DIR)\nmodel = model.merge_and_unload()\n\nmodel.save_pretrained(MERGED_DIR)\ntokenizer.save_pretrained(MERGED_DIR)\n\nprint(\"LoRA merged â†’ FP16 model saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_int8 = BitsAndBytesConfig(load_in_8bit=True)\n\nmodel_int8 = AutoModelForCausalLM.from_pretrained(\n    MERGED_DIR,\n    quantization_config=bnb_int8,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel_int8.save_pretrained(INT8_DIR)\ntokenizer.save_pretrained(INT8_DIR)\n\nprint(\"INT8 model saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_int4 = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel_int4 = AutoModelForCausalLM.from_pretrained(\n    MERGED_DIR,\n    quantization_config=bnb_int4,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel_int4.save_pretrained(INT4_DIR)\ntokenizer.save_pretrained(INT4_DIR)\n\nprint(\"INT4 model saved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/ggerganov/llama.cpp\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -r llama.cpp/requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python llama.cpp/convert_hf_to_gguf.py \\\n  quantized/merged-fp16 \\\n  --outfile quantized/model.gguf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!mkdir llama.cpp/build && cd llama.cpp/build && cmake .. && cmake --build . --config Release\n!cmake -B llama.cpp/build llama.cpp\n!cmake --build llama.cpp/build --config Release -j 8\nprint(\"Built llama.cpp\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd llama.cpp/build/bin && \\./llama-quantize \\/kaggle/working/quantized/model.gguf \\/kaggle/working/quantized/model-q4_0.gguf \\q4_0","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}